{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.optimize as opt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LeakyReLU, PReLU, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, TensorBoard, ReduceLROnPlateau\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.densenet import DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Setting the seed for numpy-generated random numbers\n",
    "np.random.seed(37)\n",
    "# Setting the seed for python random numbers\n",
    "rn.seed(1254)\n",
    "# Setting the graph-level random seed.\n",
    "tf.set_random_seed(89)\n",
    "# 自動增長 GPU 記憶體用量\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# 設定 Keras 使用的 Session\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"test1_notgenerator\"# os.path.basename(__file__).split('.')[0]\n",
    "PATH = os.getcwd()\n",
    "TRAIN = os.path.join(os.getcwd(), 'data', 'train')\n",
    "TEST = os.path.join(os.getcwd(), 'data', 'test')\n",
    "PREPROCESSED = os.path.join(os.getcwd(), 'preprocessed_data')\n",
    "LABELS = os.path.join(os.getcwd(), 'data', 'train.csv')\n",
    "SAMPLE = os.path.join(os.getcwd(), 'data', 'sample_submission.csv')\n",
    "MODEL = os.path.join(os.getcwd(), 'model', NAME+'.h5')\n",
    "RESULT = os.path.join(os.getcwd(), 'result', NAME+'_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LENGTH = 512\n",
    "IMAGE_WIDTH = 512\n",
    "CHANNEL_NUM = 4\n",
    "TRAIN_SIZE = int(len(os.listdir(TRAIN))/4)\n",
    "LABEL_NUM = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_x.npy'))\n",
    "train_y = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_y.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator to implement data augmentation. \n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range = 40,\n",
    "            width_shift_range = 0.3,\n",
    "            height_shift_range = 0.3,\n",
    "            zoom_range = [0.6, 1.4],\n",
    "            shear_range = 0.4,\n",
    "            horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    #x = Dense(299, activation='relu')(bn)\n",
    "    conv2d = Conv2D(3, kernel_size = (1, 1), strides=(1,1), padding = 'same', kernel_initializer = 'glorot_normal')(bn)\n",
    "    x = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(IMAGE_LENGTH, IMAGE_WIDTH, 3), pooling='avg')(conv2d)\n",
    "    #x = DenseNet201(include_top=False, weights='imagenet', input_shape=(IMAGE_LENGTH, IMAGE_WIDTH, 3), pooling='avg')(conv2d)\n",
    "    \n",
    "    #x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = Dense(512, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='softmax')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    input_shape=(IMAGE_LENGTH,IMAGE_WIDTH,CHANNEL_NUM), \n",
    "    n_out=LABEL_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 512, 4)       16        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 3)       15        \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Model)  (None, 1536)              54336736  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                43036     \n",
      "=================================================================\n",
      "Total params: 54,379,803\n",
      "Trainable params: 43,059\n",
      "Non-trainable params: 54,336,744\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "model.layers[-2].trainable = False\n",
    "### Show model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4)\n",
    "epochs_to_wait_for_improve = 10\n",
    "batch_size = 8\n",
    "#valid_split_ratio = 0.2\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = np.load(os.path.join(PREPROCESSED, 'class_weight.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=focal_loss(), optimizer=adam, metrics=[f1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_stopping_callback = EarlyStopping(monitor='val_f1', patience=epochs_to_wait_for_improve)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(MODEL, monitor='val_f1'\n",
    "                                          , verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OneDay\\Anaconda3\\envs\\ML2018FALL_NEW\\lib\\site-packages\\keras\\callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27964 samples, validate on 3108 samples\n",
      "Epoch 1/4\n",
      "27964/27964 [==============================] - 1408s 50ms/step - loss: 7.2936 - f1: 0.0000e+00 - val_loss: 7.3708 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 2/4\n",
      "27964/27964 [==============================] - 1399s 50ms/step - loss: 6.9794 - f1: 6.2666e-05 - val_loss: 7.5105 - val_f1: 9.1929e-05\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.00000 to 0.00009, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 3/4\n",
      "27964/27964 [==============================] - 1399s 50ms/step - loss: 6.7223 - f1: 3.1386e-04 - val_loss: 7.5923 - val_f1: 2.3595e-04\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.00009 to 0.00024, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 4/4\n",
      "27964/27964 [==============================] - 1399s 50ms/step - loss: 6.5367 - f1: 5.6151e-04 - val_loss: 7.4562 - val_f1: 8.8558e-04\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.00024 to 0.00089, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_history=model.fit(train_x, train_y\n",
    "                                      , batch_size = batch_size\n",
    "                                      #, steps_per_epoch = len(train_x)*10 / batch_size\n",
    "                                      , epochs = n_epochs\n",
    "                                      , validation_split = 0.1\n",
    "                                    , shuffle=True, class_weight=cw\n",
    "                                      , callbacks=[\n",
    "                                          #early_stopping_callback, \n",
    "                                          checkpoint_callback, TensorBoard(log_dir='./tmp/log')\n",
    "                                                   , ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=1000)\n",
    "                                                  ]\n",
    "                                      , verbose=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=./tmp/log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 512, 4)       16        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 3)       15        \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Model)  (None, 1536)              54336736  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                43036     \n",
      "=================================================================\n",
      "Total params: 54,379,803\n",
      "Trainable params: 54,319,251\n",
      "Non-trainable params: 60,552\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "### Show model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=focal_loss(), optimizer=adam, metrics=[f1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27964 samples, validate on 3108 samples\n",
      "Epoch 1/32\n",
      "27964/27964 [==============================] - 2239s 80ms/step - loss: 3.2889 - f1: 0.0743 - val_loss: 2.9206 - val_f1: 0.0918\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.07059 to 0.09176, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 2/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 2.3588 - f1: 0.1076 - val_loss: 2.6154 - val_f1: 0.1113\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.09176 to 0.11130, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 3/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 1.9172 - f1: 0.1283 - val_loss: 2.5724 - val_f1: 0.1274\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.11130 to 0.12735, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 4/32\n",
      "27964/27964 [==============================] - 2237s 80ms/step - loss: 1.6637 - f1: 0.1404 - val_loss: 2.4439 - val_f1: 0.1269\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.12735\n",
      "Epoch 5/32\n",
      "27964/27964 [==============================] - 2229s 80ms/step - loss: 1.5208 - f1: 0.1460 - val_loss: 2.3948 - val_f1: 0.1423\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.12735 to 0.14226, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 6/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 1.4021 - f1: 0.1502 - val_loss: 2.4575 - val_f1: 0.1340\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.14226\n",
      "Epoch 7/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 1.3231 - f1: 0.1516 - val_loss: 2.4285 - val_f1: 0.1427\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.14226 to 0.14274, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 8/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 1.1198 - f1: 0.1587 - val_loss: 2.2614 - val_f1: 0.1409\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.14274\n",
      "Epoch 9/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 1.0019 - f1: 0.1595 - val_loss: 2.2343 - val_f1: 0.1414\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.14274\n",
      "Epoch 10/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 0.9750 - f1: 0.1597 - val_loss: 2.1482 - val_f1: 0.1410\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.14274\n",
      "Epoch 11/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 0.9423 - f1: 0.1614 - val_loss: 2.2517 - val_f1: 0.1399\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.14274\n",
      "Epoch 12/32\n",
      "27964/27964 [==============================] - 2252s 81ms/step - loss: 0.9093 - f1: 0.1611 - val_loss: 2.2749 - val_f1: 0.1490\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.14274 to 0.14904, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test1_notgenerator.h5\n",
      "Epoch 13/32\n",
      "27964/27964 [==============================] - 2249s 80ms/step - loss: 0.8884 - f1: 0.1619 - val_loss: 2.2407 - val_f1: 0.1442\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.14904\n",
      "Epoch 14/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 0.8672 - f1: 0.1609 - val_loss: 2.3214 - val_f1: 0.1449\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.14904\n",
      "Epoch 15/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.8525 - f1: 0.1613 - val_loss: 2.2762 - val_f1: 0.1374\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.14904\n",
      "Epoch 16/32\n",
      "27964/27964 [==============================] - 2234s 80ms/step - loss: 0.8363 - f1: 0.1618 - val_loss: 2.3162 - val_f1: 0.1398\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.14904\n",
      "Epoch 17/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.8175 - f1: 0.1612 - val_loss: 2.3133 - val_f1: 0.1448\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.14904\n",
      "Epoch 18/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.8017 - f1: 0.1620 - val_loss: 2.3514 - val_f1: 0.1421\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.14904\n",
      "Epoch 19/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7981 - f1: 0.1617 - val_loss: 2.3210 - val_f1: 0.1443\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.14904\n",
      "Epoch 20/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7834 - f1: 0.1617 - val_loss: 2.3350 - val_f1: 0.1440\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.14904\n",
      "Epoch 21/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7691 - f1: 0.1619 - val_loss: 2.3952 - val_f1: 0.1438\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.14904\n",
      "Epoch 22/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7618 - f1: 0.1619 - val_loss: 2.4227 - val_f1: 0.1413\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.14904\n",
      "Epoch 23/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7600 - f1: 0.1614 - val_loss: 2.4444 - val_f1: 0.1476\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.14904\n",
      "Epoch 24/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7477 - f1: 0.1628 - val_loss: 2.3855 - val_f1: 0.1419\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.14904\n",
      "Epoch 25/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7492 - f1: 0.1613 - val_loss: 2.4529 - val_f1: 0.1411\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.14904\n",
      "Epoch 26/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7402 - f1: 0.1618 - val_loss: 2.5370 - val_f1: 0.1378\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.14904\n",
      "Epoch 27/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7407 - f1: 0.1617 - val_loss: 2.4864 - val_f1: 0.1442\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.14904\n",
      "Epoch 28/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7323 - f1: 0.1622 - val_loss: 2.4031 - val_f1: 0.1409\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.14904\n",
      "Epoch 29/32\n",
      "27964/27964 [==============================] - 2235s 80ms/step - loss: 0.7351 - f1: 0.1621 - val_loss: 2.5789 - val_f1: 0.1449\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.14904\n",
      "Epoch 30/32\n",
      "27964/27964 [==============================] - 2239s 80ms/step - loss: 0.7276 - f1: 0.1616 - val_loss: 2.5026 - val_f1: 0.1403\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.14904\n",
      "Epoch 31/32\n",
      "27964/27964 [==============================] - 2236s 80ms/step - loss: 0.7215 - f1: 0.1623 - val_loss: 2.6313 - val_f1: 0.1482\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.14904\n",
      "Epoch 32/32\n",
      "27964/27964 [==============================] - 2237s 80ms/step - loss: 0.7271 - f1: 0.1623 - val_loss: 2.5762 - val_f1: 0.1431\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.14904\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(train_x, train_y\n",
    "                                      , batch_size = batch_size\n",
    "                                      #, steps_per_epoch = len(train_x)*10 / batch_size\n",
    "                                      , epochs = n_epochs\n",
    "                                      , validation_split = 0.1\n",
    "                                    , shuffle=True, class_weight=cw\n",
    "                                      , callbacks=[\n",
    "                                          #early_stopping_callback, \n",
    "                                          checkpoint_callback, TensorBoard(log_dir='./tmp/log')\n",
    "                                                   , ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=1000)\n",
    "                                                  ]\n",
    "                                      , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML2018FALL_NEW]",
   "language": "python",
   "name": "conda-env-ML2018FALL_NEW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
