{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.optimize as opt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LeakyReLU, PReLU, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, TensorBoard\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Setting the seed for numpy-generated random numbers\n",
    "np.random.seed(37)\n",
    "# Setting the seed for python random numbers\n",
    "rn.seed(1254)\n",
    "# Setting the graph-level random seed.\n",
    "tf.set_random_seed(89)\n",
    "# 自動增長 GPU 記憶體用量\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# 設定 Keras 使用的 Session\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nname_label_dict = {\\n0:  'Nucleoplasm',\\n1:  'Nuclear membrane',\\n2:  'Nucleoli',   \\n3:  'Nucleoli fibrillar center',\\n4:  'Nuclear speckles',\\n5:  'Nuclear bodies',\\n6:  'Endoplasmic reticulum',   \\n7:  'Golgi apparatus',\\n8:  'Peroxisomes',\\n9:  'Endosomes',\\n10:  'Lysosomes',\\n11:  'Intermediate filaments',\\n12:  'Actin filaments',\\n13:  'Focal adhesion sites',   \\n14:  'Microtubules',\\n15:  'Microtubule ends',  \\n16:  'Cytokinetic bridge',   \\n17:  'Mitotic spindle',\\n18:  'Microtubule organizing center',  \\n19:  'Centrosome',\\n20:  'Lipid droplets',\\n21:  'Plasma membrane',   \\n22:  'Cell junctions', \\n23:  'Mitochondria',\\n24:  'Aggresome',\\n25:  'Cytosol',\\n26:  'Cytoplasmic bodies',   \\n27:  'Rods & rings' }\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"test1_notgenerator_TTA_thre\"# os.path.basename(__file__).split('.')[0]\n",
    "PATH = os.getcwd()\n",
    "TRAIN = os.path.join(os.getcwd(), 'data', 'train')\n",
    "TEST = os.path.join(os.getcwd(), 'data', 'test')\n",
    "PREPROCESSED = os.path.join(os.getcwd(), 'preprocessed_data')\n",
    "LABELS = os.path.join(os.getcwd(), 'data', 'train.csv')\n",
    "SAMPLE = os.path.join(os.getcwd(), 'data', 'sample_submission.csv')\n",
    "MODEL = os.path.join(os.getcwd(), 'model', NAME[:-9]+'.h5')\n",
    "RESULT = os.path.join(os.getcwd(), 'result', NAME+'_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(os.path.join(os.getcwd(), 'tta_wrapper'))\n",
    "#from wrappers import tta_segmentation\n",
    "from tta_wrapper import tta_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LENGTH = 512\n",
    "IMAGE_WIDTH = 512\n",
    "CHANNEL_NUM = 4\n",
    "#TRAIN_SIZE = int(len(os.listdir(TRAIN))/4)\n",
    "LABEL_NUM = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_thre(y, pred_y):\n",
    "    col_n = y.shape[1]\n",
    "    thre_list = []\n",
    "    for i in range(col_n):\n",
    "        print (i, end=\" \")\n",
    "        y_list = list(y[:,i])\n",
    "        pred_list = list(pred_y[:,i])\n",
    "        pred_sorted_list = sorted(pred_list)\n",
    "        median_list = [min(pred_sorted_list)-0.005]\n",
    "        \n",
    "        # calculate probably threshold\n",
    "        for p in range(len(pred_sorted_list)-1):\n",
    "            median_list.append( (pred_sorted_list[p]+pred_sorted_list[p+1])/2 )\n",
    "        median_list.append(max(pred_sorted_list)+0.005)\n",
    "        \n",
    "        # find the threshold to maximun accuracy\n",
    "        best_acc = 0\n",
    "        for m in median_list:\n",
    "            \n",
    "            tf_list = list(np.array(y_list) == np.array([ 1 if pre>=m else 0 for pre in pred_list]))\n",
    "            tf_list = [1 if t else 0 for t in tf_list]\n",
    "            acc = sum(tf_list)/len(tf_list)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_thre = m\n",
    "                    \n",
    "        thre_list.append(best_thre)\n",
    "    return thre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.name is 'nt':\n",
    "    from multiprocessing.dummy import Pool\n",
    "else:\n",
    "    from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_np(y_pred, y_true, threshold=0.5):\n",
    "    '''numpy f1 metric'''\n",
    "    y_pred = (y_pred>threshold).astype(int)\n",
    "    TP = (y_pred*y_true).sum(1)\n",
    "    prec = TP/(y_pred.sum(1)+1e-7)\n",
    "    rec = TP/(y_true.sum(1)+1e-7)\n",
    "    res = 2*prec*rec/(prec+rec+1e-7)\n",
    "    return res.mean()\n",
    "\n",
    "\n",
    "def f1_n(y_pred, y_true, thresh, n, default=0.5):\n",
    "    '''partial f1 function for index n'''\n",
    "    threshold = default * np.ones(y_pred.shape[1])\n",
    "    threshold[n]=thresh\n",
    "    return f1_np(y_pred, y_true, threshold)\n",
    "\n",
    "def sub_find(args):\n",
    "    y_pred = args[0]\n",
    "    y_true = args[1]\n",
    "    th = args[2]\n",
    "    i = args[3]\n",
    "    aux = f1_n(y_pred, y_true, th, i)\n",
    "    return aux\n",
    "\n",
    "def find_thresh(y_pred, y_true):\n",
    "    '''brute force thresh finder'''\n",
    "    ths = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        args = []\n",
    "        for th in np.linspace(0, 1, 1000):\n",
    "            args.append((y_pred, y_true, th, i))\n",
    "        with Pool() as p:\n",
    "            aux = p.map(sub_find, args)\n",
    "        \n",
    "#         aux = []\n",
    "#         for th in np.linspace(0, 1, 1000):\n",
    "#             aux += [f1_n(y_pred, y_true, th, i)]\n",
    "        ths += [np.array(aux).argmax() / 1000]\n",
    "    return np.array(ths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = np.load(os.path.join(PREPROCESSED, 'valid_RGBY_original_x.npy'))\n",
    "valid_y = np.load(os.path.join(PREPROCESSED, 'valid_RGBY_original_y.npy'))\n",
    "#valid_x = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_x.npy'))\n",
    "#valid_y = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_y.npy'))\n",
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "model = load_model(MODEL, custom_objects={'f1': f1, 'focal_loss_fixed' : focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108/3108 [==============================] - 516s 166ms/step\n"
     ]
    }
   ],
   "source": [
    "tta_model = tta_classification(model, h_flip=True, rotation=(90, 270), \n",
    "                             merge='mean')\n",
    "\n",
    "\n",
    "valid_pred_y = tta_model.predict(valid_x, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3108, 512, 512, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01114885"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(valid_pred_y[:,-1])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#thre_list = calcu_thre(valid_y,valid_pred_y)\n",
    "thre_list = find_thresh(valid_pred_y, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.139, 0.076, 0.103, 0.092, 0.115, 0.081, 0.099, 0.055, 0.09 ,\n",
       "       0.02 , 0.015, 0.094, 0.107, 0.059, 0.064, 0.043, 0.055, 0.016,\n",
       "       0.06 , 0.054, 0.056, 0.131, 0.102, 0.166, 0.07 , 0.149, 0.091,\n",
       "       0.012])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(PREPROCESSED, NAME+'.npy'), thre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML2018FALL_NEW]",
   "language": "python",
   "name": "conda-env-ML2018FALL_NEW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
