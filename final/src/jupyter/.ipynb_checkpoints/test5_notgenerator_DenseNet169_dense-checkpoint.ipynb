{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.optimize as opt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LeakyReLU, PReLU, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, TensorBoard, ReduceLROnPlateau\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.densenet import DenseNet201, DenseNet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Setting the seed for numpy-generated random numbers\n",
    "np.random.seed(37)\n",
    "# Setting the seed for python random numbers\n",
    "rn.seed(1254)\n",
    "# Setting the graph-level random seed.\n",
    "tf.set_random_seed(89)\n",
    "# 自動增長 GPU 記憶體用量\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# 設定 Keras 使用的 Session\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"test5_notgenerator_DenseNet169_dense\"# os.path.basename(__file__).split('.')[0]\n",
    "PATH = os.getcwd()\n",
    "TRAIN = os.path.join(os.getcwd(), 'data', 'train')\n",
    "TEST = os.path.join(os.getcwd(), 'data', 'test')\n",
    "PREPROCESSED = os.path.join(os.getcwd(), 'preprocessed_data')\n",
    "LABELS = os.path.join(os.getcwd(), 'data', 'train.csv')\n",
    "SAMPLE = os.path.join(os.getcwd(), 'data', 'sample_submission.csv')\n",
    "MODEL = os.path.join(os.getcwd(), 'model', NAME+'.h5')\n",
    "RESULT = os.path.join(os.getcwd(), 'result', NAME+'_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LENGTH = 512\n",
    "IMAGE_WIDTH = 512\n",
    "CHANNEL_NUM = 4\n",
    "TRAIN_SIZE = int(len(os.listdir(TRAIN))/4)\n",
    "LABEL_NUM = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_x.npy'))\n",
    "train_y = np.load(os.path.join(PREPROCESSED, 'train_RGBY_original_y.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator to implement data augmentation. \n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range = 40,\n",
    "            width_shift_range = 0.3,\n",
    "            height_shift_range = 0.3,\n",
    "            zoom_range = [0.6, 1.4],\n",
    "            shear_range = 0.4,\n",
    "            horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    \n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = Dense(CHANNEL_NUM)(bn)\n",
    "    x = Dense(3)(x)\n",
    "    #conv2d = Conv2D(3, kernel_size = (1, 1), strides=(1,1), padding = 'same', kernel_initializer = 'glorot_normal')(bn)\n",
    "    \n",
    "    #x = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(IMAGE_LENGTH, IMAGE_WIDTH, 3), pooling='avg')(conv2d)\n",
    "    x = DenseNet169(include_top=False, weights='imagenet', input_shape=(IMAGE_LENGTH, IMAGE_WIDTH, 3), pooling='avg')(x)\n",
    "    \n",
    "    #x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n",
    "    #x = Flatten()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    #x = Dense(512, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='softmax')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "    input_shape=(IMAGE_LENGTH,IMAGE_WIDTH,CHANNEL_NUM), \n",
    "    n_out=LABEL_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 512, 4)       16        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512, 512, 4)       20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512, 512, 3)       15        \n",
      "_________________________________________________________________\n",
      "densenet169 (Model)          (None, 1664)              12642880  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 28)                46620     \n",
      "=================================================================\n",
      "Total params: 12,689,551\n",
      "Trainable params: 46,663\n",
      "Non-trainable params: 12,642,888\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.layers[-2].trainable = False\n",
    "### Show model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4)\n",
    "epochs_to_wait_for_improve = 10\n",
    "batch_size = 4\n",
    "#valid_split_ratio = 0.2\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = np.load(os.path.join(PREPROCESSED, 'class_weight.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=focal_loss(), optimizer=adam, metrics=[f1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_stopping_callback = EarlyStopping(monitor='val_f1', patience=epochs_to_wait_for_improve)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(MODEL, monitor='val_f1'\n",
    "                                          , verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OneDay\\Anaconda3\\envs\\ML2018FALL_NEW\\lib\\site-packages\\keras\\callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27964 samples, validate on 3108 samples\n",
      "Epoch 1/4\n",
      "27964/27964 [==============================] - 1516s 54ms/step - loss: 3.3340 - f1: 2.5713e-04 - val_loss: 4.0083 - val_f1: 0.0024\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.00240, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 2/4\n",
      "27964/27964 [==============================] - 1488s 53ms/step - loss: 3.0173 - f1: 0.0016 - val_loss: 4.3045 - val_f1: 0.0056\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.00240 to 0.00555, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 3/4\n",
      "27964/27964 [==============================] - 1486s 53ms/step - loss: 2.8836 - f1: 0.0030 - val_loss: 4.2536 - val_f1: 0.0057\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.00555 to 0.00568, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 4/4\n",
      "27964/27964 [==============================] - 1486s 53ms/step - loss: 2.8105 - f1: 0.0040 - val_loss: 4.3115 - val_f1: 0.0063: 2.8129 - - ETA: 54s - loss: 2.8131  - ETA: 52 - ETA: 38s - loss: 2.8124 - f1:  - ETA: 37s - loss: 2.812 - ETA - ETA\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.00568 to 0.00631, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_history=model.fit(train_x, train_y\n",
    "                                      , batch_size = batch_size\n",
    "                                      , epochs = n_epochs\n",
    "                                      , validation_split = 0.1\n",
    "                                    , shuffle=True, class_weight=cw\n",
    "                                      , callbacks=[ \n",
    "                                          checkpoint_callback, TensorBoard(log_dir='./tmp/log')\n",
    "                                                   , ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001)\n",
    "                                                  ]\n",
    "                                      , verbose=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=./tmp/log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 512, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 512, 4)       16        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512, 512, 4)       20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512, 512, 3)       15        \n",
      "_________________________________________________________________\n",
      "densenet169 (Model)          (None, 1664)              12642880  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 28)                46620     \n",
      "=================================================================\n",
      "Total params: 205,071\n",
      "Trainable params: 46,663\n",
      "Non-trainable params: 158,408\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OneDay\\Anaconda3\\envs\\ML2018FALL_NEW\\lib\\site-packages\\keras\\engine\\training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "### Show model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=focal_loss(), optimizer=adam, metrics=[f1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27964 samples, validate on 3108 samples\n",
      "Epoch 1/20\n",
      "27964/27964 [==============================] - 2234s 80ms/step - loss: 2.4006 - f1: 0.0182 - val_loss: 2.3571 - val_f1: 0.0437\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.00631 to 0.04374, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 2/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 1.9579 - f1: 0.0312 - val_loss: 2.2104 - val_f1: 0.0577\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.04374 to 0.05768, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 3/20\n",
      "27964/27964 [==============================] - 2204s 79ms/step - loss: 1.7114 - f1: 0.0409 - val_loss: 1.8473 - val_f1: 0.0559\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.05768\n",
      "Epoch 4/20\n",
      "27964/27964 [==============================] - 2220s 79ms/step - loss: 1.5115 - f1: 0.0499 - val_loss: 2.0626 - val_f1: 0.0653\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.05768 to 0.06532, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 5/20\n",
      "27964/27964 [==============================] - 2228s 80ms/step - loss: 1.3622 - f1: 0.0578 - val_loss: inf - val_f1: 0.0633\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.06532\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/20\n",
      "27964/27964 [==============================] - 2215s 79ms/step - loss: 1.0911 - f1: 0.0719 - val_loss: 1.7967 - val_f1: 0.0802\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.06532 to 0.08023, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 7/20\n",
      "27964/27964 [==============================] - 2226s 80ms/step - loss: 0.9692 - f1: 0.0782 - val_loss: 1.7979 - val_f1: 0.0788\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.08023\n",
      "Epoch 8/20\n",
      "27964/27964 [==============================] - 2231s 80ms/step - loss: 0.9136 - f1: 0.0813 - val_loss: 2.0082 - val_f1: 0.0855\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.08023 to 0.08555, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/20\n",
      "27964/27964 [==============================] - 2228s 80ms/step - loss: 0.7961 - f1: 0.0877 - val_loss: 1.7043 - val_f1: 0.0844\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.08555\n",
      "Epoch 10/20\n",
      "27964/27964 [==============================] - 2232s 80ms/step - loss: 0.7559 - f1: 0.0892 - val_loss: 1.6582 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.08555 to 0.08707, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 11/20\n",
      "27964/27964 [==============================] - 2226s 80ms/step - loss: 0.7205 - f1: 0.0906 - val_loss: 1.6495 - val_f1: 0.0843\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.08707\n",
      "Epoch 12/20\n",
      "27964/27964 [==============================] - 2199s 79ms/step - loss: 0.7021 - f1: 0.0912 - val_loss: 1.6677 - val_f1: 0.0868\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.08707\n",
      "Epoch 13/20\n",
      "27964/27964 [==============================] - 2187s 78ms/step - loss: 0.6895 - f1: 0.0918 - val_loss: 1.6956 - val_f1: 0.0862\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.08707\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 14/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.6505 - f1: 0.0936 - val_loss: 1.6654 - val_f1: 0.0862\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.08707\n",
      "Epoch 15/20\n",
      "27964/27964 [==============================] - 2193s 78ms/step - loss: 0.6214 - f1: 0.0942 - val_loss: 1.6789 - val_f1: 0.0872\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.08707 to 0.08716, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 16/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.6055 - f1: 0.0947 - val_loss: 1.6429 - val_f1: 0.0869\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.08716\n",
      "Epoch 17/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.5950 - f1: 0.0947 - val_loss: 1.5912 - val_f1: 0.0878\n",
      "\n",
      "Epoch 00017: val_f1 improved from 0.08716 to 0.08784, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 18/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.5888 - f1: 0.0948 - val_loss: 1.6304 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.08784\n",
      "Epoch 19/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.5823 - f1: 0.0948 - val_loss: 1.6440 - val_f1: 0.0865\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.08784\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 20/20\n",
      "27964/27964 [==============================] - 2194s 78ms/step - loss: 0.5764 - f1: 0.0952 - val_loss: 1.6029 - val_f1: 0.0868\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.08784\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(train_x, train_y\n",
    "                                      , batch_size = batch_size\n",
    "                                      #, steps_per_epoch = len(train_x)*10 / batch_size\n",
    "                                      , epochs = n_epochs\n",
    "                                      , validation_split = 0.1\n",
    "                                    , shuffle=True, class_weight=cw\n",
    "                                      , callbacks=[\n",
    "                                          #early_stopping_callback, \n",
    "                                          checkpoint_callback, TensorBoard(log_dir='./tmp/log')\n",
    "                                                   , ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.00001)\n",
    "                                                  ]\n",
    "                                      , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27964 samples, validate on 3108 samples\n",
      "Epoch 1/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 0.5610 - f1: 0.0955 - val_loss: 1.6045 - val_f1: 0.0877\n",
      "\n",
      "Epoch 00001: val_f1 did not improve from 0.08784\n",
      "Epoch 2/20\n",
      "27964/27964 [==============================] - 2198s 79ms/step - loss: 0.5647 - f1: 0.0954 - val_loss: 1.5987 - val_f1: 0.0870\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.08784\n",
      "Epoch 3/20\n",
      "27964/27964 [==============================] - 2204s 79ms/step - loss: 0.5626 - f1: 0.0949 - val_loss: 1.5960 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.08784\n",
      "Epoch 4/20\n",
      "27964/27964 [==============================] - 2211s 79ms/step - loss: 0.5587 - f1: 0.0954 - val_loss: 1.6345 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.08784\n",
      "Epoch 5/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 0.5531 - f1: 0.0957 - val_loss: 1.6233 - val_f1: 0.0868\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.08784\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 6/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5532 - f1: 0.0955 - val_loss: 1.5779 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.08784\n",
      "Epoch 7/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 0.5516 - f1: 0.0951 - val_loss: 1.5835 - val_f1: 0.0862\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.08784\n",
      "Epoch 8/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 0.5519 - f1: 0.0955 - val_loss: 1.5925 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.08784\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 9/20\n",
      "27964/27964 [==============================] - 2197s 79ms/step - loss: 0.5546 - f1: 0.0948 - val_loss: 1.5821 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.08784\n",
      "Epoch 10/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5475 - f1: 0.0954 - val_loss: 1.5662 - val_f1: 0.0861\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.08784\n",
      "Epoch 11/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5412 - f1: 0.0951 - val_loss: 1.5842 - val_f1: 0.0880\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.08784 to 0.08798, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "Epoch 12/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5471 - f1: 0.0953 - val_loss: 1.5569 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.08798\n",
      "Epoch 13/20\n",
      "27964/27964 [==============================] - 2195s 79ms/step - loss: 0.5445 - f1: 0.0951 - val_loss: 1.5669 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.08798\n",
      "Epoch 14/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5421 - f1: 0.0955 - val_loss: 1.5701 - val_f1: 0.0875\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.08798\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 15/20\n",
      "27964/27964 [==============================] - 2195s 79ms/step - loss: 0.5481 - f1: 0.0958 - val_loss: 1.5869 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.08798\n",
      "Epoch 16/20\n",
      "27964/27964 [==============================] - 2195s 79ms/step - loss: 0.5389 - f1: 0.0955 - val_loss: 1.6018 - val_f1: 0.0880\n",
      "\n",
      "Epoch 00016: val_f1 improved from 0.08798 to 0.08804, saving model to C:\\Users\\OneDay\\Downloads\\ML2018FALL\\final\\Human_Protein_Atlas_Image_classification\\model\\test5_notgenerator_DenseNet169_dense.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "Epoch 17/20\n",
      "27964/27964 [==============================] - 2195s 79ms/step - loss: 0.5345 - f1: 0.0955 - val_loss: 1.5752 - val_f1: 0.0870\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.08804\n",
      "Epoch 18/20\n",
      "27964/27964 [==============================] - 2195s 79ms/step - loss: 0.5434 - f1: 0.0952 - val_loss: 1.5651 - val_f1: 0.0877\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.08804\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "Epoch 19/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5393 - f1: 0.0955 - val_loss: 1.5555 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.08804\n",
      "Epoch 20/20\n",
      "27964/27964 [==============================] - 2196s 79ms/step - loss: 0.5386 - f1: 0.0959 - val_loss: 1.5706 - val_f1: 0.0877\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.08804\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(train_x, train_y\n",
    "                                      , batch_size = batch_size\n",
    "                                      #, steps_per_epoch = len(train_x)*10 / batch_size\n",
    "                                      , epochs = n_epochs\n",
    "                                      , validation_split = 0.1\n",
    "                                    , shuffle=True, class_weight=cw\n",
    "                                      , callbacks=[\n",
    "                                          #early_stopping_callback, \n",
    "                                          checkpoint_callback, TensorBoard(log_dir='./tmp/log')\n",
    "                                                   , ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.00001)\n",
    "                                                  ]\n",
    "                                      , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML2018FALL_NEW]",
   "language": "python",
   "name": "conda-env-ML2018FALL_NEW-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
